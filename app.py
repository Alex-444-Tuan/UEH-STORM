import os # module to interact with the operating system
from dotenv import load_dotenv # module to load environment variables from a .env file
from langchain_groq import ChatGroq
import chainlit as cl
from utils.stormv2 import run_storm_pipeline
from langchain.callbacks.tracers import LangChainTracer
import asyncio
from langsmith import Client
import json
import re
from typing import List, Tuple


load_dotenv()


# d√πng ƒë·ªÉ chia table content d·ª±a tr√™n # ·ªü trong file storm_gen_article.txt
def parse_all_markdown_sections(text: str) -> List[Tuple[str, str]]:
    pattern = r"(#+) (.+?)\n(.*?)(?=\n#+ |\Z)"
    matches = re.findall(pattern, text, flags=re.DOTALL)

    if not matches:
        return [("To√†n b·ªô n·ªôi dung", text.strip())]

    sections = []
    for hashes, raw_title, content in matches:
        level = len(hashes)
        prefix = "-" * level  # d√πng - theo s·ªë c·∫•p
        display_title = f"{prefix} {raw_title.strip()}"
        full_content = f"{hashes} {raw_title.strip()}\n\n{content.strip()}"
        sections.append((display_title, full_content))

    return sections



# client = Client(
#   api_key= os.getenv("LANGSMITH_API_KEY"), 
#   api_url="https://api.smith.langchain.com",  
# ) 
# # ƒë√¢y l√† optional c√≥ th·ªÉ l∆∞·ª£t b·ªè b·ªõt



@cl.on_message
async def handle_message(msg: cl.Message):

    await cl.Message(content='Preparing the STORM Wiki pipeline...').send()

    topic = msg.content
    await cl.Message(content=f"Topic received: `{topic}`\nRunning the pipeline...").send()


    try:
        await cl.sleep(0.3)
        # ch·∫°y Pipeline Storm
        # asyncio d√πng ƒë·ªÉ ch·∫°y pipeline ·ªü m·ªôt block ri√™ng bi·ªát
        # ƒë·ªÉ kh√¥ng l√†m gi√°n ƒëo·∫°n qu√° tr√¨nh x·ª≠ l√Ω c·ªßa Chainlit
        await asyncio.to_thread(run_storm_pipeline,
            topic=topic,
            retriever="you",
            output_dir="./data",
            do_research=True,
            do_generate_outline=True,
            do_generate_article=True,
            do_polish_article=True
        )

        await asyncio.sleep(0.5)

        # Ki·ªÉm tra xem file ƒë√£ ƒë∆∞·ª£c t·∫°o ra ch∆∞a
        result_path = f"./data/{topic.replace(' ', '_').replace('/', '_')}/storm_gen_article.txt"
        if os.path.exists(result_path):
            with open(result_path, "r") as f:
                article = f.read()
            await cl.Message(content=f"‚úÖ Article generated:\n\n{article}").send()
        else:
            await cl.Message(content="‚ö†Ô∏è Article file not found!").send()

        sections = parse_all_markdown_sections(article)

        cl.user_session.set("sections", sections)  # Store sections in user session
        cl.user_session.set("topic", topic)  # Store topic in user session  

        toc = cl.TaskList(title=f"Table of Contents for {topic}", tasks=[cl.Task(title=title,status="done") for title, _ in sections])

        await toc.send()

        url_info_path = f"./data/{topic.replace(' ', '_').replace('/', '_')}/url_to_info.json"

        with open(url_info_path, "r") as f:
            full_info = json.load(f)

        # ki·ªÉm tra file link url 
        url_map = full_info.get("url_to_unified_index", {})

        if not url_map:
            await cl.Message(content="‚ö†Ô∏è `url_to_unified_index` not found or empty in `url_to_info.json`.").send()
        else:
            # Sort by index (value)
            sorted_urls = sorted(url_map.items(), key=lambda x: x[1])

            # l·∫≠p table content
            markdown_links = "\n".join(
                f"{i+1}. [{url}]({url})" for i, (url, _) in enumerate(sorted_urls)
            )

            await cl.Message(content="üîó **Sources used in article (from `url_to_info.json`)**:\n" + markdown_links).send()

    except Exception as e:
        await cl.Message(content=f" Error running STORM pipeline:\n```\n{str(e)}\n```").send()

    